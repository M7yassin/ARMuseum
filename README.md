Final Report – An Augmented Reality System for Museum
1. Introduction
This report details the design and implementation of an augmented reality (AR) application developed in Unity for the Android platform. The system is designed to enhance a museum visitor's experience by recognizing 2D paintings and augmenting them with interactive 3D characters. The core technical objectives were to implement a stable image tracking system, ensure realistic placement of virtual content onto physical surfaces, and create an immersive user experience where the augmented content is aware of the viewer's position.
The 3D character assets and their animations were sourced from Adobe Mixamo.
2. System Setup and Core Technologies
The foundation of the project was built upon several key packages from the Unity Package Manager, specifically chosen for Android AR development:
•	AR Foundation: The core Unity framework that provides a unified API for AR development. While it supports cross-platform development, it was utilized here to specifically target the Android ecosystem.
•	ARCore XR Plugin: This is the platform-specific backend that serves as the bridge between AR Foundation and Google's native AR library (ARCore) for Android devices.
•	TextMeshPro: A powerful text rendering solution used to display high-quality descriptions for the 3D characters.
3. AR System Architecture and Implementation
The application's logic is built around the core subsystems provided by AR Foundation. My custom scripts interface with these subsystems to create the desired interactive experience.
3.1. Image Tracking Subsystem The primary function of this subsystem is to recognize and track real-world 2D images.
•	Tracking Configuration: The process begins by populating an XRReferenceImageLibrary with the target images (the paintings). These images are pre-processed to extract feature points, enabling them to be used as visual markers.
•	Detection and Tracking Loop: In the live application, the ARTrackedImageManager is the component responsible for the tracking loop. It continuously analyzes the device's camera feed, comparing it against the feature points in the reference library.
•	Event-Driven Spawning: When a marker is successfully detected, the ARTrackedImageManager fires a trackedImagesChanged event. The main controller script, CharacterManagerARF.cs, subscribes to this event to manage the spawning process. To ensure modularity and ease of management, each character was prepared as a self-contained Prefab. This Prefab includes the 3D model, its UI Canvas for the text, and all necessary scripts (RotateWithCam, LookCam). Upon detecting a new image, the script instantiates the corresponding Prefab. To manage the active characters in the scene, a Dictionary data structure is used. When a character is spawned, it is added to the Dictionary with the image name as the key. This prevents the same character from being instantiated multiple times and provides an efficient way to access and hide the character when its corresponding image is no longer tracked.

3.2. Plane Detection and Content Placement Subsystem For realistic augmentation, virtual objects must appear to be part of the user's environment. This requires an understanding of the real-world geometry.
•	Surface Detection and Filtering: The ARPlaneManager component works in the background to detect flat surfaces (planes) in the user's environment. To improve the reliability of this system, PlaneClassifier.cs is used to filter out undesirable planes (e.g., vertical walls) by deactivating them as soon as they are detected, ensuring only valid ground surfaces are considered.
•	Content Placement: To place the character on a ground plane, the CharacterManagerARF.cs script implements a straightforward placement method. It iterates through all active planes and calculates the horizontal (X/Z) distance between the tracked image and the center of each plane. The character is then placed at the X and Z coordinates of the tracked image, but at the Y-height of the plane that was found to be closest. This approach grounds the content on the nearest detected surface.
3.3. User Interaction and Immersion To make the experience feel interactive, the virtual content must react to the user's presence.
•	Character and UI Rotation: This is achieved through two specialized scripts. RotateWithCam.cs is attached to the 3D character model and rotates it to face the user's camera, but constrained to the Y-axis to prevent unnatural tilting. Meanwhile, LookCam.cs is attached to the UI Canvas containing the text description, allowing it to fully rotate and face the user, ensuring readability from any angle.
4. Conclusion
This project successfully implements a stable AR experience for the Android platform by leveraging the core subsystems of AR Foundation. The modular architecture, with distinct scripts for management (CharacterManagerARF.cs), filtering (PlaneClassifier.cs), and user interaction (RotateWithCam.cs, LookCam.cs), results in a clean and maintainable codebase. 
